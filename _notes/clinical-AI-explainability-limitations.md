---
title: AI explainability in clinical settings is only marginally useful
date: 2024-05-28
tags: healthcare AI
pinned: true
description: This blog post explores the limitations and challenges of machine learning explainability in clinical settings. It discusses the historical context and current regulations driving the need for explainable AI in healthcare, highlighting real-world examples where explainability methods like SHAP fall short. The post examines the disparity between what clinicians need from AI explanations and what they actually receive, shedding light on the democratization of explainability tools and the potential dangers of oversimplified explanations. It also considers alternative approaches, such as leveraging AI for collaborative workflows, and reflects on the future of explainability methods in improving clinical decision-making.
---

We all know the blurb when it comes to explainability of machine learning models in healthcare. Clinicians need to understand the reasons behind predictions when these models aid care delivery. This need has been top-of-mind since the early days of machine learning in clinical settings, since atleast 1991, when [William Baxt used a neural network to predict myocardial infarction](https://www.acpjournals.org/doi/abs/10.7326/0003-4819-115-11-843?journalCode=aim) in the emergency department. Now, in 2024, the European Union has passed a regulatory framework for AI systems, requiring providers to offer explainability modules and meticulously document their use.

Despite all the progress in explainable AI, its current state is still lacking. Don't take my word for it - the sentiment is shared by both ML researchers and clinicians using these systems in practice. For example, [a system tested at UPMC for treatment recommendations](https://dl.acm.org/doi/pdf/10.1145/3544548.3581075) found clinicians quick to dismiss predictions when they couldn't reconcile the model's explanation with their understanding of evidence-based practices. One clinician even said, "...she’s not hypotensive. So why in the world is the AI asking me to start pressors? I’m rapidly losing faith in Sepsis AI". It wouldn't be the first time.

Explainable AI in clinical settings has been underwhelming and potentially unreliable for two main reasons. First, current methods often fall short in providing the kind of explanations that clinicians need. Second, explainability has been democratized into off-the-shelf open-source tools. The inner workings have been abstracted away, meaning AI product developers and clinical data scientists no longer need to deeply understand the intricacies of the methods (I'll admit that I'm guilty of this). As a result, developers and data scientists lack the mental models necessary to effectively communicate limitations to clinicians.

Interfaces displaying predictions to end-users are fairly standardized. There is some outcome of interest, such as an adverse event like hospitalization or onset of disease, and prediction scores assigned to patients by the model. Along with each patient's score, there is a list of "features" (also known as variables, predictors, or factors) determined to be influential to the model - the so-called "contributing factors" to the prediction.

To highlight "contributing factors", AI developers have rallied around one particular method called [SHAP](https://papers.nips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf). SHAP is part of a larger family of methods for determining "variable importance," which itself falls under the broader category of model explainability. SHAP became mainstream around 2019 primarily because of its breakthroughs in computational efficiency and the [high-quality software](https://shap.readthedocs.io/en/latest/) released by its creators.

DataRobot (a platform for building AI models for various use cases), [describes SHAP's role](https://docs.datarobot.com/en/docs/modeling/analyze-models/understand/pred-explain/shap-pe.html):

> SHAP-based explanations help identify the impact on the decision for each factor.

The problems begin immediately. What does it mean for something to "impact" a black-box model?

## Clinician's expectations and the reality

Before we dive in, let's think about the kind of explanation a clinician needs from a black-box. Imagine a clinician in the emergency department who gets an alert from a machine learning model indicating a patient is at immediate risk of cardiac arrest, a condition the clinician hadn't considered for this patient. If an all-knowing oracle were available, it may say something like: "The ECG results show signs of ventricular tachycardia and ST-segment elevations, indicating a high risk of cardiac arrest." Ideally, we want our machine learning model to provide a similar explanation.

The clinician needs to understand the model's behavior in the context of the patient, especially when the model contradicts their intuition or judgment. They also need to [reconcile the model's behavior with evidence-based practices](https://arxiv.org/pdf/1905.05134). This isn't just about accountability; it's necessary for identifying when the model's prediction is wrong. Models might be more accurate than human judgment in many clinical use cases, but they're far from always being correct.

There's little consensus on the exact question a model's explanation should answer. Should we be able to answer, "Why did the model predict outcome Y for this patient?" [Or is there a simpler explanation that will suffice](https://arxiv.org/pdf/2002.11097)? [Some argue that the only satisfactory explanation is a causal account](https://link.springer.com/article/10.1007/s11229-022-03485-5) of why the model made a particular prediction: "Why did the model predict outcome Y instead of alternative Z for this patient?" This opens a can of worms about what makes a good explanation, which I'll leave to the logicians and philosophers. We might find that SHAP is inconsistent in describing even simple model behavior, let alone distinguishing between different outcomes.

Let's compare this with what DataRobot says about converting SHAP values to statements about model behavior.

> The [SHAP] contribution describes how much each listed feature is responsible for pushing the target away from the average.
>
> ...
>
> They answer why a model made a certain prediction—What drives a customer's decision to buy—age? gender? buying habits?

So higher SHAP values correspond to features that drive the prediction higher, while lower SHAP values correspond to features that drive it lower. It seems like we should be able to tell our clinician: "_The model predicted outcome X because of features A, B, and C_."

ML experts are finding that this way of converting SHAP values to model explanations is incomplete and even dangerous. One reason is that there are edge cases where SHAP fails to capture basic model behavior.

- Even if a feature isn't directly used by the model, [SHAP might still show it as important](https://arxiv.org/pdf/1909.08128) because the feature is correlated to other important features. For example, [folks from UW trained a model to predict mortality using CDC data](https://arxiv.org/pdf/2006.16234). The final model they chose made no use of the patient's BMI, one of the columns in the CDC data. Despite this, SHAP assigned BMI a positive importance because BMI is correlated to other features used by the model. This happens because [SHAP splits importance between correlated features](https://arxiv.org/pdf/1903.10464).
- As a result of SHAP splitting importance between correlated features, [features that the model relies on heavily might appear less important to SHAP](https://arxiv.org/pdf/2303.05981), leading to confusion about their true impact.
- [SHAP doesn't tell us how a model's predictions will change if we adjust feature values](https://arxiv.org/pdf/2212.11870). For example, a Stanford study used ML to predict survival outcomes in a clinical trial based on patient eligibility criteria. The author's conclude that "Shapley values close to zero [...] correspond to eligibility criteria that had no effect on [...] the overall survival." In other words, they expected that changing these criteria wouldn't affect the prediction. However, theoretical results and experiments show that SHAP values don't reliably reliably indicate how the model's output will change with feature adjustments. A large positive SHAP value doesn't mean increasing that feature will increase the prediction. If these findings are correct, we might never reliably answer, "Why did the model choose outcome Y instead of alternative Z?" because we can't guarantee that changing SHAP-identified features would change the model's output.

These issues with SHAP's behavior raise many questions about its usefulness. Are DataRobot's documents simply incorrect? How often do we encounter edge cases in production models? Can data scientists identify these cases before deploying a model? Before tackling these questions, it's helpful to consider some perspectives that ML researchers have been advocating for a while, but are often overlooked by AI developers creating products.

SHAP and related explainability methods provide post-hoc, rationales for black-box model predictions. These rationales are generated from approximations of the black-box model, and as a result of being approximations may stray far from replicating the model's original calculations. For this reason, the term "explanation" itself might be misleading. A more accurate term that reflects what these explainability methods are doing is providing a "[summary of prediction trends](https://arxiv.org/pdf/1811.10154)."

As a result of being approximations to the true model, we have to accept that explanations will be wrong. Explanations will never perfectly replicate the model's behavior for all patients. If they could, we wouldn't need the original model. Even if an explanation method correctly captures model behavior for 90% of patients, it will still be wrong for 10%. Additionally, it's tough to quantify how often the explanation will be wrong, as this requires understanding the model's behavior, which is what the explanation is trying to achieve. It's also hard to know when the explanation is wrong or for which types of patients it fails to capture the model's behavior. Some might argue that in high-stakes environments, this makes any explanation method, and therefore black-box models, unreliable.

To see how these perspectives apply to SHAP, let's briefly look at how SHAP values are computed. SHAP values determine feature importance by considering how the model behaves if certain patient features were "unknown". By turning features on and off in different combinations, you can see which features have the most impact on the prediction. How features are made "unknown" is a technical detail of SHAP and one of its key innovations. To get a true evaluation of model behavior in the absence of certain features, you would need to retrain the model numerous times, atleast once for every feature. SHAP bypasses this computational infeasability by approximating the model's behavior under unknown features, which I suppose is both its innovation as an explainability method as well as the crux of its limitations.

As I mentioned before, the creators of SHAP released excellent software, allowing practitioners to use it without needing to understand the intricate details. However, SHAP is not a simple algorithm. It is based on principles from game theory developed by Nobel Prize winner Lloyd Shapley and includes several detailed algorithmic choices with significant implications. Despite SHAP's mainstream adoption, it is often assumed to be a complete solution when, in reality, it is still a work in progress. Again, I'll admit my own guilt here. ML researchers are still exploring its limitations and developing new explainability methods. There is concern that these methods might only be used correctly by algorithmic experts. Average users might fall into confirmation bias and develop incomplete mental models, leading to overtrusting the tool.

Another example. [In 2015, Mount Sinai used an ML model to triage pneumonia patients](https://link.springer.com/article/10.1007/s11229-022-03485-5). The model had excellent accuracy, but they eventually noticed something peculiar -- it assigned a low probability of mortality to patients with asthma, which contradicted clinical intuition. Had they used SHAP values, they would've found that the presence of asthma drives the prediction score down, and clinicians might have said something like: "The model predicts a low mortality score for this patient because they have asthma." They later discovered that historically, asthmatics with pneumonia were sent to the ICU due to their high risk, where the extra attention they received reduced their overall probability of death.

Though this issue reflects a problem with ML models rather than explainability methods specifically, it shows a human tendency with explainability methods in counterintuitive situations. In these cases, confusing results are sometimes dismissed, especially in clinical environments where models are expected to provide new insights into disease management or physiology. [People tend to rationalize that machine learning models process data differently from humans and rely on "a lot of complex math" to uncover things we might not fully understand](https://harmanpk.github.io/Papers/CHI2020_Interpretability.pdf). In the case of Mount Sinai, this could have been dangerous.

Effective data science teams vet their models for explainability, involving the clinical team using the model. Still, I don't see how its possible for all failure cases can be identified before deployment.

Some researchers advocate abandoning black-box models in favor of simpler, naturally interpretable models. Others suggest using multiple explainability methods together, leveraging their strengths and weaknesses in a way that allows end-users to interact with the system. However, this approach seems impractical in a clinical environment, where no clinician would want to spend 15 minutes inside a portal reviewing comprehensive explainability modules with multiple interfaces.

[Some](https://arxiv.org/pdf/1811.10154) [researchers](https://borisbabic.com/research/beware_march2023.pdf) have called for abandoning black-box models in favor of simpler, naturally interpretable models. [Others](https://link.springer.com/article/10.1007/s11229-022-03485-5) [suggest](https://arxiv.org/pdf/2007.04131) using multiple explainability methods together, leveraging their strengths and weaknesses, in a way that allows the end-user to interact with an end-system. I don't see how such a system would be feasible in a clinical envrionment - no clinician would want to spend 15 minutes inside a portal reviewing comprehensive explainability modules with multiple interfaces. Perhaps with multi-agent LLMs becoming more robust, we will see a system where these LLMs, much like how they are used to write code and build apps with a human-in-the-loop, can use multiple explainability methods through an interactive interface to provide a deeper understanding of model behavior.

For now, however, we must ask whether predictive models lose all value if we cannot trust their explanations.

## Potential directions

Whatever qualms you may have about explainability, there is no denying that ML offers benefits in accuracy compared to clinicians when it comes to predicting outcomes.

How can we leverage this benefit without depending on explainability? Lessons learned from [a deployment done by Stanford offers a new perspective](https://scottsdaleinstitute.org/wp-content/uploads/2021/12/Using-AI-to-Empower_2022.pdf).

In 2022, Stanford deployed AI systems at Stanford Health used for surfacing seriously ill hospitalized patients who may benefit from advanced care planning (ACP). ACP helps clinicians identify patients who might benefit from palliative or hospice care. From their previous experience with clinical AI systems and feedback from clinicians, the authors knew the concerns clinicians have about AI, like how clinicians often disagree with ML predictions or feel the AI doesn't provide new insights.

Stanford, in this deployment, emphasized AI's utility in structuring collaborative workflows, rather than being correct or offering new insights. The AI model acts as a "mediator", helping clinicians and nurses adopt a shared mental model of risk. The AI serves as an objective, initial assessor of risk. The motivation for this was an issue that they surfaced from stakeholder interviews - clinicians often noted that, in multidisciplinary teams, there is hesitation to take action due to disagreements between physicians and nurses assessment of a patients risk, which can lead to missed ACP opportunities.

Again, the AI's role here is not about delivering detailed insights or ensuring agreement on individual predictions. In fact, the alerts generated by Stanford's model do not even mention contributing factors. The AI model is just the first step in assessing risk. The alert triggers an assessment protocol—a "serious illness conversation guide"—where a nurse assesses the patient's needs and goals, deciding on additional care management initiatives.

The approach still allows better allocation of resources by directing nurses to patients who would benefit most from ACP, addressing the issue of limited nursing resources and avoiding unnecessary ACP for patients who do not need it.
